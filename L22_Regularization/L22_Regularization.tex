\documentclass[8pt,aspectratio=169]{beamer}
\usetheme{Madrid}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{amsmath}

\definecolor{mlpurple}{RGB}{51,51,178}
\definecolor{mllavender}{RGB}{173,173,224}
\definecolor{mllavender3}{RGB}{204,204,235}

\setbeamercolor{palette primary}{bg=mllavender3,fg=mlpurple}
\setbeamercolor{structure}{fg=mlpurple}
\setbeamercolor{frametitle}{fg=mlpurple,bg=mllavender3}

\setbeamertemplate{navigation symbols}{}
\setbeamersize{text margin left=5mm,text margin right=5mm}

\newcommand{\bottomnote}[1]{\vfill\footnotesize\textbf{#1}}


\title{Lesson 22: Regularization}
\subtitle{Data Science with Python -- BSc Course}
\date{45 Minutes}

\begin{document}

\begin{frame}[plain]
\titlepage
\end{frame}

\begin{frame}[t]{Learning Objectives}
\textbf{The Problem:}
With many predictors, our model can memorize noise instead of learning patterns.
How do we build models that generalize to unseen data?

\vspace{0.3em}
\textbf{After this lesson, you will be able to:}
\begin{itemize}
\item Recognize overfitting and its causes
\item Apply Ridge (L2) regularization to shrink coefficients
\item Apply Lasso (L1) for automatic feature selection
\item Tune the regularization strength with cross-validation
\end{itemize}
\bottomnote{Finance Application: Building robust factor models with many correlated predictors}
\end{frame}


\begin{frame}[t]{Underfitting: Model Too Simple}
\textbf{When Models Miss the Pattern}
\begin{itemize}
\item Constant prediction ignores relationship in data
\item High error on both training and test sets
\end{itemize}
\begin{center}
\includegraphics[width=0.55\textwidth]{01a_underfitting/chart.pdf}
\end{center}
\bottomnote{Underfitting = high bias. The model is too simple to capture the true pattern.}
\end{frame}


\begin{frame}[t]{Good Fit: Right Complexity}
\begin{center}
\includegraphics[width=0.65\textwidth]{01b_good_fit/chart.pdf}
\end{center}
\bottomnote{The goal: A model complex enough to capture patterns, simple enough to generalize}
\end{frame}


\begin{frame}[t]{Overfitting: Model Too Complex}
\textbf{When Models Memorize Noise}
\begin{itemize}
\item Perfect fit on training data, poor on new data
\item High-degree polynomials chase every point
\end{itemize}
\begin{center}
\includegraphics[width=0.55\textwidth]{01c_overfitting/chart.pdf}
\end{center}
\bottomnote{Overfitting = high variance. Model changes drastically with different training samples.}
\end{frame}


\begin{frame}[t]{Bias-Variance Tradeoff}
\begin{center}
\includegraphics[width=0.65\textwidth]{01d_bias_variance/chart.pdf}
\end{center}
\bottomnote{Regularization helps find the optimal complexity -- not too simple, not too complex}
\end{frame}


\begin{frame}[t]{Ridge Regression (L2)}
\textbf{Shrink All Coefficients Toward Zero}
\begin{itemize}
\item Add penalty: $\text{Loss} = \sum(y - \hat{y})^2 + \lambda\sum\beta_j^2$
\item Large $\lambda$ = stronger shrinkage, simpler model
\end{itemize}
\begin{center}
\includegraphics[width=0.55\textwidth]{02_ridge_concept/chart.pdf}
\end{center}
\bottomnote{Ridge keeps all features but reduces their influence -- good for multicollinearity}
\end{frame}


\begin{frame}[t]{Lasso Regression (L1)}
\textbf{Some Coefficients Go to Exactly Zero}
\begin{itemize}
\item Add penalty: $\text{Loss} = \sum(y - \hat{y})^2 + \lambda\sum|\beta_j|$
\item L1 penalty creates sparse solutions (automatic feature selection)
\end{itemize}
\begin{center}
\includegraphics[width=0.55\textwidth]{03_lasso_concept/chart.pdf}
\end{center}
\bottomnote{Lasso eliminates irrelevant features -- use when you suspect many predictors are useless}
\end{frame}


\begin{frame}[t]{Ridge Coefficient Path}
\textbf{Smooth Shrinkage Toward Zero}
\begin{itemize}
\item All coefficients shrink as $\lambda$ increases
\item Coefficients approach but never reach zero
\end{itemize}
\begin{center}
\includegraphics[width=0.55\textwidth]{04a_ridge_path/chart.pdf}
\end{center}
\bottomnote{Ridge keeps all features in the model -- good when all predictors may be relevant}
\end{frame}


\begin{frame}[t]{Lasso Coefficient Path}
\textbf{Sparse Feature Selection}
\begin{itemize}
\item Coefficients hit exactly zero at different $\lambda$ values
\item Weaker predictors are eliminated first
\end{itemize}
\begin{center}
\includegraphics[width=0.55\textwidth]{04b_lasso_path/chart.pdf}
\end{center}
\bottomnote{Lasso automatically selects the most important features}
\end{frame}


\begin{frame}[t]{Lasso Feature Count}
\begin{center}
\includegraphics[width=0.65\textwidth]{04c_feature_count/chart.pdf}
\end{center}
\bottomnote{As $\lambda$ increases, fewer features remain -- use CV to find optimal sparsity}
\end{frame}


\begin{frame}[t]{Tuning Lambda}
\textbf{Finding the Right Penalty Strength}
\begin{itemize}
\item Too small: overfitting (model too complex)
\item Too large: underfitting (model too simple)
\end{itemize}
\begin{center}
\includegraphics[width=0.55\textwidth]{05_lambda_tuning/chart.pdf}
\end{center}
\bottomnote{Use cross-validation to find the lambda that minimizes test error}
\end{frame}


\begin{frame}[t]{Cross-Validation for Lambda}
\textbf{sklearn Makes It Easy}
\begin{itemize}
\item \texttt{RidgeCV} and \texttt{LassoCV} automatically search lambda values
\item K-fold CV: split data K ways, train on K-1, test on 1, average
\end{itemize}
\begin{center}
\includegraphics[width=0.55\textwidth]{06_cross_validation/chart.pdf}
\end{center}
\bottomnote{Rule: Use \texttt{RidgeCV(alphas=[0.1, 1, 10, 100])} to search logarithmically}
\end{frame}


\begin{frame}[t]{Feature Selection with Lasso}
\textbf{Which Predictors Actually Matter?}
\begin{itemize}
\item Non-zero Lasso coefficients = selected features
\item Zero coefficients = features eliminated by the model
\end{itemize}
\begin{center}
\includegraphics[width=0.55\textwidth]{07_feature_selection/chart.pdf}
\end{center}
\bottomnote{Finance insight: Lasso often keeps 3-5 factors from a candidate set of 20+}
\end{frame}


\begin{frame}[t]{Finance Application: Factor Models}
\textbf{Building Robust Return Predictions}
\begin{itemize}
\item Many candidate factors are correlated (value, quality, momentum...)
\item Regularization prevents unstable, extreme factor weights
\end{itemize}
\begin{center}
\includegraphics[width=0.55\textwidth]{08_finance_regularization/chart.pdf}
\end{center}
\bottomnote{Industry practice: Regularized regression for combining alpha signals}
\end{frame}


\begin{frame}[t]{Hands-On Exercise (25 min)}
\textbf{Task: Compare Ridge vs Lasso on Multi-Factor Data}
\begin{enumerate}
\item Create synthetic data with 20 features (only 5 are truly predictive)
\item Fit OLS, Ridge, and Lasso models
\item Compare test set $R^2$ for each model
\item Plot Lasso coefficients -- which features were selected?
\end{enumerate}

\vspace{0.3em}
\textbf{Deliverable:} Bar chart of coefficients comparing OLS vs Ridge vs Lasso.
\bottomnote{Extension: Use LassoCV to find optimal lambda and report selected features}
\end{frame}


\begin{frame}[t]{Lesson Summary}
\textbf{Problem Solved:}
Regularization prevents overfitting when we have many predictors or limited data.

\vspace{0.3em}
\textbf{Key Takeaways:}
\begin{itemize}
\item Ridge (L2) shrinks all coefficients -- handles multicollinearity
\item Lasso (L1) sets some coefficients to zero -- automatic feature selection
\item Use cross-validation (RidgeCV, LassoCV) to tune $\lambda$
\end{itemize}

\vspace{0.3em}
\textbf{Next Lesson:} Regression Metrics (L23) -- how do we measure model quality?
\bottomnote{Memory: Ridge = Ridge keeps all features. Lasso = Lasso Loses features (L for Lose).}
\end{frame}

\end{document}
